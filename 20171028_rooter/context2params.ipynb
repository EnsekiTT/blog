{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 音声のコンテキストからパラメータを推測する\n",
    "## 諸々Import (CPUで動くはず)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import optimizers\n",
    "from chainer import serializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コンテキストとパラメータの学習データを読み込む\n",
    "問題設定: コンテキストの情報からパラメータを予測する\n",
    "\n",
    "これができると何ができるのか？: \n",
    "* 形態素解析などでコンテキストを作成する\n",
    "* コンテキストからパラメータを推測する←今日ここ\n",
    "* パラメータから音声を作成する\n",
    "* うわあああああしゃべったあああああああ\n",
    "-----\n",
    "メモ\n",
    "* コンテキストとは、先行、当該、後続、アクセント句長、アクセント句位置、アクセント型、フレーム数の7次元からなる。\n",
    "* パラメータとは、基本周波数とメルケプストラムの27次元からなる。\n",
    "(本来、コンテキストに対してパラメータは可変長になるが、長さの予測は行わず、学習時に対応するフレーム数を導入することで対応関係を取ることにした。)\n",
    "\n",
    "### 学習・評価データの形式\n",
    "* [先行, 当該, 後続, アクセント句長, アクセント句位置,　アクセント型, フレーム数,　基本周波数, メルケプストラム[0...25]]\n",
    "* [phoneme0, phoneme1, phoneme2, accent_len, accent_pos,　accent_type, frame_num,　f0, mcep[0...25]]\n",
    "\n",
    "### 使ってみるときの形式\n",
    "* [先行, 当該, 後続, アクセント句長, アクセント句位置,　アクセント型, フレーム数]\n",
    "* [phoneme0, phoneme1, phoneme2, accent_len, accent_pos,　accent_type, frame_num]\n",
    "\n",
    "### 使ってみた時に出てくる形式\n",
    "* [基本周波数, メルケプストラム[0...25]]\n",
    "* [f0, mcep[0...25]]\n",
    "\n",
    "### datasディレクトリに諸々入れて使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 音素の変換テーブル\n",
    "#　モデルの読み込みに使う。\n",
    "with open('onso.pickle', 'rb') as i:\n",
    "    onso_table = pickle.load(i)\n",
    "# 学習データとテストデータを読み込む\n",
    "\n",
    "try:\n",
    "    contexts_train_csv = pd.read_csv('datas/train.csv', header=None) # ←ここに学習用のテキストデータを渡すと\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "\n",
    "test_path = \"datas/UEM_091.lab\" # ←ここにUEM_091.labを渡すと UEM_091.lab.paramが生成される\n",
    "try:\n",
    "    contexts_test_csv = pd.read_csv(test_path, header=None) \n",
    "except FileNotFoundError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phoneme0</th>\n",
       "      <th>phoneme1</th>\n",
       "      <th>phoneme2</th>\n",
       "      <th>accent_len</th>\n",
       "      <th>accent_pos</th>\n",
       "      <th>accent_type</th>\n",
       "      <th>frame_num</th>\n",
       "      <th>f0</th>\n",
       "      <th>mcep0</th>\n",
       "      <th>mcep1</th>\n",
       "      <th>...</th>\n",
       "      <th>mcep16</th>\n",
       "      <th>mcep17</th>\n",
       "      <th>mcep18</th>\n",
       "      <th>mcep19</th>\n",
       "      <th>mcep20</th>\n",
       "      <th>mcep21</th>\n",
       "      <th>mcep22</th>\n",
       "      <th>mcep23</th>\n",
       "      <th>mcep24</th>\n",
       "      <th>mcep25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.648539</td>\n",
       "      <td>0.456265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019452</td>\n",
       "      <td>0.004019</td>\n",
       "      <td>-0.012505</td>\n",
       "      <td>-0.003931</td>\n",
       "      <td>0.045686</td>\n",
       "      <td>0.116717</td>\n",
       "      <td>-0.151013</td>\n",
       "      <td>-0.115716</td>\n",
       "      <td>-0.163912</td>\n",
       "      <td>-0.080456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.433629</td>\n",
       "      <td>0.324613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047829</td>\n",
       "      <td>0.130855</td>\n",
       "      <td>-0.063560</td>\n",
       "      <td>0.033953</td>\n",
       "      <td>0.030672</td>\n",
       "      <td>-0.013612</td>\n",
       "      <td>-0.234040</td>\n",
       "      <td>-0.031978</td>\n",
       "      <td>-0.100765</td>\n",
       "      <td>-0.225542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.493183</td>\n",
       "      <td>0.337381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113755</td>\n",
       "      <td>0.164137</td>\n",
       "      <td>-0.175271</td>\n",
       "      <td>0.068837</td>\n",
       "      <td>0.074774</td>\n",
       "      <td>-0.096105</td>\n",
       "      <td>-0.062341</td>\n",
       "      <td>0.062833</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>-0.156493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.537887</td>\n",
       "      <td>0.220762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134378</td>\n",
       "      <td>0.168410</td>\n",
       "      <td>-0.067635</td>\n",
       "      <td>0.114970</td>\n",
       "      <td>0.170909</td>\n",
       "      <td>-0.030111</td>\n",
       "      <td>0.040802</td>\n",
       "      <td>-0.067689</td>\n",
       "      <td>-0.100786</td>\n",
       "      <td>-0.049495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.501573</td>\n",
       "      <td>0.188340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033314</td>\n",
       "      <td>0.019883</td>\n",
       "      <td>0.075013</td>\n",
       "      <td>0.149588</td>\n",
       "      <td>0.076710</td>\n",
       "      <td>0.116201</td>\n",
       "      <td>0.102792</td>\n",
       "      <td>0.035638</td>\n",
       "      <td>-0.067440</td>\n",
       "      <td>0.040577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   phoneme0  phoneme1  phoneme2  accent_len  accent_pos  accent_type  \\\n",
       "0        29        14         1           2           1            0   \n",
       "1        29        14         1           2           1            0   \n",
       "2        29        14         1           2           1            0   \n",
       "3        29        14         1           2           1            0   \n",
       "4        29        14         1           2           1            0   \n",
       "\n",
       "   frame_num   f0     mcep0     mcep1    ...       mcep16    mcep17    mcep18  \\\n",
       "0          1  0.0 -0.648539  0.456265    ...     0.019452  0.004019 -0.012505   \n",
       "1          2  0.0 -0.433629  0.324613    ...     0.047829  0.130855 -0.063560   \n",
       "2          3  0.0 -0.493183  0.337381    ...     0.113755  0.164137 -0.175271   \n",
       "3          4  0.0 -0.537887  0.220762    ...     0.134378  0.168410 -0.067635   \n",
       "4          5  0.0 -0.501573  0.188340    ...    -0.033314  0.019883  0.075013   \n",
       "\n",
       "     mcep19    mcep20    mcep21    mcep22    mcep23    mcep24    mcep25  \n",
       "0 -0.003931  0.045686  0.116717 -0.151013 -0.115716 -0.163912 -0.080456  \n",
       "1  0.033953  0.030672 -0.013612 -0.234040 -0.031978 -0.100765 -0.225542  \n",
       "2  0.068837  0.074774 -0.096105 -0.062341  0.062833  0.002481 -0.156493  \n",
       "3  0.114970  0.170909 -0.030111  0.040802 -0.067689 -0.100786 -0.049495  \n",
       "4  0.149588  0.076710  0.116201  0.102792  0.035638 -0.067440  0.040577  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_len = contexts_train_csv.shape[0]\n",
    "contexts_train = pd.DataFrame()\n",
    "contexts_train['phoneme0'] = [onso_table[r] for r in contexts_train_csv[0]]\n",
    "contexts_train['phoneme1'] = [onso_table[r] for r in contexts_train_csv[1]]\n",
    "contexts_train['phoneme2'] = [onso_table[r] for r in contexts_train_csv[2]]\n",
    "contexts_train['accent_len'] = contexts_train_csv[3]\n",
    "contexts_train['accent_pos'] = contexts_train_csv[4]\n",
    "contexts_train['accent_type'] = contexts_train_csv[5]\n",
    "contexts_train['frame_num'] = contexts_train_csv[6] # \n",
    "contexts_train['f0'] = contexts_train_csv[7]\n",
    "for i in range(26):\n",
    "    contexts_train['mcep'+str(i)] = contexts_train_csv[8+i]\n",
    "\n",
    "contexts_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phoneme0</th>\n",
       "      <th>phoneme1</th>\n",
       "      <th>phoneme2</th>\n",
       "      <th>accent_len</th>\n",
       "      <th>accent_pos</th>\n",
       "      <th>accent_type</th>\n",
       "      <th>frame_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phoneme0  phoneme1  phoneme2  accent_len  accent_pos  accent_type  \\\n",
       "0        29         5        16           5           1            3   \n",
       "1        29         5        16           5           1            3   \n",
       "2        29         5        16           5           1            3   \n",
       "3        29         5        16           5           1            3   \n",
       "4        29         5        16           5           1            3   \n",
       "\n",
       "   frame_num  \n",
       "0          1  \n",
       "1          2  \n",
       "2          3  \n",
       "3          4  \n",
       "4          5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_len = contexts_test_csv.shape[0]\n",
    "contexts_test = pd.DataFrame()\n",
    "contexts_test['phoneme0'] = [onso_table[r] for r in contexts_test_csv[0]]\n",
    "contexts_test['phoneme1'] = [onso_table[r] for r in contexts_test_csv[1]]\n",
    "contexts_test['phoneme2'] = [onso_table[r] for r in contexts_test_csv[2]]\n",
    "contexts_test['accent_len'] = contexts_test_csv[3]\n",
    "contexts_test['accent_pos'] = contexts_test_csv[4]\n",
    "contexts_test['accent_type'] = contexts_test_csv[5]\n",
    "contexts_test['frame_num'] = contexts_test_csv[6]\n",
    "contexts_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークを定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare network\n",
    "# n_inは入力ベクトル長\n",
    "# n_outは出力ベクトル長\n",
    "# 良い手段かわからないけど倍ずつ大きくして、最後27次元に落とす\n",
    "# 一応毎回BatchNormalizationする。\n",
    "class context2params(chainer.Chain):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(context2params, self).__init__(\n",
    "            l1=L.Linear(n_in, 14),\n",
    "            l2=L.Linear(14, 28),\n",
    "            l3=L.Linear(28, 56),\n",
    "            l4=L.Linear(56, n_out),\n",
    "            bn1 = L.BatchNormalization(14),\n",
    "            bn2 = L.BatchNormalization(28),\n",
    "            bn3 = L.BatchNormalization(56),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.bn1(self.l1(x)))\n",
    "        h2 = F.relu(self.bn2(self.l2(h1)))\n",
    "        h3 = F.relu(self.bn3(self.l3(h2)))\n",
    "        y = self.l4(h3)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・評価データセットを切り分ける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 80%を学習に使って20％を学習の評価に使う。\n",
    "contexts_train_sample = contexts_train.sample(frac=1)\n",
    "N = int(len(contexts_train)*0.8)\n",
    "N_test = len(contexts_train)-N\n",
    "x_train = contexts_train.iloc[:N, 0:7].values.astype(np.float32)\n",
    "x_test = contexts_train.iloc[N:, 0:7].values.astype(np.float32)\n",
    "y_train = contexts_train.iloc[:N, 7:].values.astype(np.float32)\n",
    "y_test = contexts_train.iloc[N:, 7:].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークとオプティマイザを定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ネットワーク\n",
    "model = context2params(7, 27)\n",
    "\n",
    "# 最適化\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100\n",
      "train mean loss=24.847913259124674\n",
      "test  mean loss=12.577761710316084\n",
      "epoch 200\n",
      "train mean loss=23.770819652173298\n",
      "test  mean loss=16.00720233572316\n",
      "epoch 300\n",
      "train mean loss=23.331794393994432\n",
      "test  mean loss=18.414956975229437\n",
      "epoch 400\n",
      "train mean loss=23.422317709914182\n",
      "test  mean loss=19.838232393009577\n",
      "epoch 500\n",
      "train mean loss=22.814166543631238\n",
      "test  mean loss=22.862010248390114\n",
      "epoch 600\n",
      "train mean loss=22.256477625780455\n",
      "test  mean loss=23.847220174646118\n",
      "epoch 700\n",
      "train mean loss=22.245688806802946\n",
      "test  mean loss=24.848305787291896\n",
      "epoch 800\n",
      "train mean loss=21.12741128452549\n",
      "test  mean loss=29.084618519760152\n",
      "epoch 900\n",
      "train mean loss=21.898663744768125\n",
      "test  mean loss=25.91465725162996\n",
      "epoch 1000\n",
      "train mean loss=21.96635589642778\n",
      "test  mean loss=28.690377397596073\n",
      "epoch 1100\n",
      "train mean loss=21.628385439136085\n",
      "test  mean loss=33.4604271771652\n",
      "epoch 1200\n",
      "train mean loss=21.65906163380565\n",
      "test  mean loss=27.908760132836353\n",
      "epoch 1300\n",
      "train mean loss=21.182312542699265\n",
      "test  mean loss=35.41176308701674\n"
     ]
    }
   ],
   "source": [
    "# 一度に勾配を求めるバッチのサイズ\n",
    "batchsize = 100\n",
    "# 学習を行う回数\n",
    "n_epoch = 10000\n",
    "\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    # 学習\n",
    "    # バッチサイズに合わせてランダムに切り出す\n",
    "    perm = np.random.permutation(N)\n",
    "    sum_loss = 0\n",
    "    for i in range(0, N, batchsize):\n",
    "        # バッチづくり\n",
    "        x = np.asarray(x_train[perm[i:i + batchsize]])\n",
    "        t = np.asarray(y_train[perm[i:i + batchsize]])\n",
    "\n",
    "        # 予測\n",
    "        p = model(x)\n",
    "        \n",
    "        # 誤差の計算\n",
    "        loss = F.mean_squared_error(p, t)\n",
    "\n",
    "        # 誤差を使って最適化\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        \n",
    "        # 表示用\n",
    "        sum_loss += float(loss.data) * len(t.data)\n",
    "    train_loss = sum_loss\n",
    "\n",
    "    # Epoch毎の評価\n",
    "    sum_loss = 0\n",
    "    for i in range(0, N_test, batchsize):\n",
    "        #　バッチづくり\n",
    "        x = np.asarray(x_test[i:i + batchsize])\n",
    "        t = np.asarray(y_test[i:i + batchsize])\n",
    "        \n",
    "        # 予測\n",
    "        p = model(x)\n",
    "        \n",
    "        # 誤差の計算\n",
    "        loss = F.mean_squared_error(p, t)\n",
    "        sum_loss += float(loss.data) * len(t.data)\n",
    "    test_loss = sum_loss\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch', epoch)\n",
    "        print('train mean loss={}'.format(train_loss / N))\n",
    "        print('test  mean loss={}'.format(test_loss / N_test))\n",
    "        model.to_cpu()\n",
    "        # 破壊的なので注意\n",
    "        with open('datas/context2params_{0:07d}.pickle'.format(epoch), 'wb') as o:\n",
    "            pickle.dump(model, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを保存しておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GPUで動かしていたらCPUに戻してくる\n",
    "model.to_cpu()\n",
    "# 破壊的なので注意\n",
    "with open('datas/context2params.pickle', 'wb') as o:\n",
    "    pickle.dump(model, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#　モデルの読み込みに使う。\n",
    "with open('datas/context2params.pickle', 'rb') as i:\n",
    "    model = pickle.load(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 実際の使い方\n",
    "# contexts_testからnumpy 配列として読み出してmodelに渡す\n",
    "x = contexts_test.values.astype(np.float32)\n",
    "# modelの返り値はVariable型になっているので.dataでnumpy.array型に戻す。\n",
    "pred = pd.DataFrame(model(x).data, dtype=np.float32)\n",
    "\n",
    "# 音声生成が使いやすい形式のファイルに書き出す\n",
    "# CSV\n",
    "pred.to_csv(test_path+'.param', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0 for i in range(7)]], dtype=np.float32)\n",
    "plt.bar(range(27), model(x).data[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最初は持ってないから疑似学習・評価データを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 疑似データ生成\n",
    "train_data_len = 50\n",
    "np.random.seed(0)\n",
    "contexts_train = pd.DataFrame()\n",
    "contexts_train['phoneme0'] = np.random.randint(0,20, size = train_data_len)\n",
    "contexts_train['phoneme1'] = np.random.randint(0,20, size = train_data_len)\n",
    "contexts_train['phoneme2'] = np.random.randint(0,20, size = train_data_len)\n",
    "contexts_train['accent_len'] = np.random.randint(0,20, size = train_data_len)\n",
    "contexts_train['accent_pos'] = np.random.randint(0,20, size = train_data_len)\n",
    "contexts_train['accent_type'] = np.random.randint(0,20, size = train_data_len)\n",
    "contexts_train['frame_num'] = np.random.randint(0,20, size = train_data_len)\n",
    "contexts_train['f0'] = np.random.rand(train_data_len)\n",
    "for i in range(26):\n",
    "    contexts_train['mcep'+str(i)] = np.random.rand(train_data_len)\n",
    "\n",
    "contexts_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最初は持ってないから疑似テストデータを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_len = 1000\n",
    "contexts_test = pd.DataFrame()\n",
    "contexts_test['phoneme0'] = np.random.randint(0,20, size = test_data_len)\n",
    "contexts_test['phoneme1'] = np.random.randint(0,20, size = test_data_len)\n",
    "contexts_test['phoneme2'] = np.random.randint(0,20, size = test_data_len)\n",
    "contexts_test['accent_len'] = np.random.randint(0,20, size = test_data_len)\n",
    "contexts_test['accent_pos'] = np.random.randint(0,20, size = test_data_len)\n",
    "contexts_test['accent_type'] = np.random.randint(0,20, size = test_data_len)\n",
    "contexts_test['frame_num'] = np.random.randint(0,20, size = test_data_len)\n",
    "\n",
    "contexts_test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
